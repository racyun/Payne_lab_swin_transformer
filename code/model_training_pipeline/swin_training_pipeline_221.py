# -*- coding: utf-8 -*-
"""swin_training_pipeline_221.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GysI44stBQBRhAbzFwf8Jv0FRwRBSRuk

# Swin Transformer Training Pipeline

This Colab mounts Google Drive, loads paired carbonate images and their segmentation masks, and defines a custom PyTorch dataset that matches image files to mask files, converts them to the right format, and normalizes them for training. It then builds a semantic-segmentation model (UPerNet) with a pretrained SwinV2 backbone from Hugging Face Transformers, and trains it on an 80/20 train/validation split using AdamW. After each epoch it evaluates performance (validation loss, pixel accuracy, and mean IoU) and saves the best-performing model checkpoint. Finally, it runs the trained model on a validation image and generates a quick visualization showing the predicted mask (and an overlay on the original image).
"""

!pip -q install --upgrade transformers
!pip -q install tqdm

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torch.optim as optim
import numpy as np
from pathlib import Path
from tqdm.auto import tqdm

from torchvision.io import read_image
from torchvision import tv_tensors
from torchvision.transforms.v2 import functional as V2F   # <-- v2 image ops
import torch.nn.functional as F

from transformers import (
    UperNetForSemanticSegmentation,
    UperNetConfig,
    Swinv2Config,
)

import matplotlib.pyplot as plt
from torchvision.io import read_image

image = read_image("/content/drive/carbonate_imgs_and_masks/img/32_TK-176.2 (2x detail bivalves).jpg")
mask = read_image("/content/drive/carbonate_imgs_and_masks/masks/32_TK-176.2 (2x detail bivalves).png")

# print(mask.unique)


plt.figure(figsize=(16, 8))
plt.subplot(121)
plt.title("Image")
plt.imshow(image.permute(1, 2, 0))
plt.subplot(122)
plt.title("Mask")
plt.imshow(mask.permute(1, 2, 0))

# all_equal = (mask[1] == mask[2]).all()
# print(all_equal)

# unique_vals = torch.unique(mask)
# print(unique_vals)
print(mask.shape)
# print(mask.unique)
mask_np = mask.numpy()
# print(mask_np[0][0])


# np.unique(mask_np)

unique_values, counts = np.unique(mask_np, return_counts=True)

# Display the results
print("Unique Values:", unique_values)
print("Counts:", counts)

class carbonate_segmentation_dataset(torch.utils.data.Dataset):

    IMAGENET_MEAN = (0.485, 0.456, 0.406)
    IMAGENET_STD  = (0.229, 0.224, 0.225)
    IMG_EXTS  = {".jpg"}
    MASK_EXTS = {".png"}

    def __init__(self, root, transforms, normalize=True, strict=True):
        self.root = Path(root)
        self.transforms = transforms
        self.normalize = normalize
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(root, "img"))))
        self.masks = list(sorted(os.listdir(os.path.join(root, "masks"))))

        #---------------------- FIX STARTS HERE----------------------------------------
        img_dir  = self.root / "img"
        mask_dir = self.root / "masks"

        imgs  = {p.stem: p for p in img_dir.iterdir()
                 if p.is_file() and p.suffix.lower() in self.IMG_EXTS and p.name[0] != "."}
        masks = {p.stem: p for p in mask_dir.iterdir()
                 if p.is_file() and p.suffix.lower() in self.MASK_EXTS and p.name[0] != "."}

        common = sorted(imgs.keys() & masks.keys())
        self.pairs = [(imgs[k], masks[k]) for k in common]

        # Helpful diagnostics for mismatches
        missing_masks = sorted(imgs.keys() - masks.keys())
        missing_imgs  = sorted(masks.keys() - imgs.keys())
        if strict:
            if missing_masks:
                print(f"[dataset] {len(missing_masks)} images have no mask. Examples:", missing_masks[:5])
            if missing_imgs:
                print(f"[dataset] {len(missing_imgs)} masks have no image. Examples:", missing_imgs[:5])
            if not self.pairs:
                raise RuntimeError("Found no (image, mask) pairs. Check folder names & extensions.")
        print(f"[dataset] Using {len(self.pairs)} paired samples.")
        # -----------------------------------------------------------------------

    def __getitem__(self, idx):
        # load images and masks
        img_path, mask_path = self.pairs[idx]
        img = read_image(img_path)
        mask = read_image(mask_path)

        if mask.ndim == 3 and mask.shape[0] > 1:
          mask = mask[0:1, ...]

        mask = mask.squeeze(0)
        mask = mask.to(torch.long)

        img = tv_tensors.Image(img)
        sem_mask = tv_tensors.Mask(mask)

        if self.transforms is not None:
          img, sem_mask = self.transforms(img, sem_mask)

        # -----------------ADDED THIS PART---------------------
        img_f = V2F.convert_image_dtype(img, dtype=torch.float32)  # [C, H, W], float32 in [0,1]

        # ensure 3 channels (replicate grayscale)
        if img_f.shape[0] == 1:
            img_f = img_f.repeat(3, 1, 1)

        if self.normalize:
            img_f = V2F.normalize(img_f, mean=self.IMAGENET_MEAN, std=self.IMAGENET_STD)

        labels = torch.as_tensor(sem_mask, dtype=torch.long)
        return img_f, labels
        # -----------------------------------------------------

    def __len__(self):
        return len(self.pairs)

import torchvision
import torch.nn as nn
from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights

NUM_CLASSES = 21

# load a model pre-trained on COCO
model = deeplabv3_resnet50(
    weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1,
    num_classes=NUM_CLASSES,
    aux_loss=True,  # True if you want aux head/loss
)

criterion = nn.CrossEntropyLoss()

NUM_CLASSES = 21
IGNORE_INDEX = 255
LR = 5e-4
WEIGHT_DECAY = 1e-4

def create_upernet_swinv2(num_classes, ignore_index = 255):
    """Build UPerNet with a SwinV2 backbone.
    - We ask UPerNet to use a Transformer-provided backbone with pretrained weights.
    - Decoder/head is randomly initialized for your num_classes.
    """
    cfg = UperNetConfig(
        # backbone_config=back_cfg, # use SwinV2 as backbone
        backbone="microsoft/swinv2-tiny-patch4-window8-256",
        use_pretrained_backbone=True, # load SwinV2 pretrained weights
        backbone_kwargs={"out_indices": [0, 1, 2, 3]},
        num_labels=num_classes,
        loss_ignore_index=ignore_index,
        use_auxiliary_head=False,
    )
    model = UperNetForSemanticSegmentation(cfg)
    return model

model = create_upernet_swinv2(
    num_classes=NUM_CLASSES,
    ignore_index=IGNORE_INDEX,
)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

from transformers import UperNetForSemanticSegmentation, UperNetConfig

def get_model_semantic_segmentation(num_classes, ignore_index=255,
                                    backbone_id="microsoft/swinv2-tiny-patch4-window8-256"):
    """
    Build a semantic segmentation model:
    - Backbone: SwinV2 (pretrained), returning 4 scales
    - Decoder: UPerNet
    - Output: per-pixel class logits with num_classes channels
    """
    cfg = UperNetConfig(
        backbone="microsoft/swinv2-tiny-patch4-window8-256",
        use_pretrained_backbone=True,
        backbone_kwargs={"out_indices": [0, 1, 2, 3]},
        num_labels=num_classes,
        loss_ignore_index=ignore_index,
        use_auxiliary_head=False,   # keep simple/robust
    )
    model = UperNetForSemanticSegmentation(cfg)
    return model

model = get_model_semantic_segmentation(NUM_CLASSES, IGNORE_INDEX)

from torch.utils.data import DataLoader, Subset
from torchvision.transforms.v2 import Compose, RandomHorizontalFlip, RandomVerticalFlip, RandomCrop, CenterCrop

# --- simple v2 transforms that work with tv_tensors.Image/Mask ---
CROP = 512  # bump to 768 if VRAM allows

train_transforms = Compose([
    RandomHorizontalFlip(p=0.5),
    RandomVerticalFlip(p=0.2),
    RandomCrop((CROP, CROP)),
])

val_transforms = Compose([
    CenterCrop((CROP, CROP)),
])

DATA_ROOT = "/content/drive/My Drive/carbonate_imgs_and_masks"

# 1) Build one base dataset just to get the full, paired list (no transforms here)
base_ds = carbonate_segmentation_dataset(DATA_ROOT, transforms=None, normalize=True)  # prints once
n = len(base_ds)

# 2) Make a reproducible 80/20 split of indices
val_frac = 0.20
n_val = max(1, int(val_frac * n))
perm = torch.randperm(n, generator=torch.Generator().manual_seed(1337))
val_idx = perm[:n_val].tolist()
train_idx = perm[n_val:].tolist()

# 3) Build two *separate* datasets with their own transforms
#    (pass strict=False to suppress duplicate pairing prints)
train_full = carbonate_segmentation_dataset(DATA_ROOT, transforms=None, normalize=True, strict=False)
val_full   = carbonate_segmentation_dataset(DATA_ROOT, transforms=None,   normalize=True, strict=False)

# 4) Subset them with the split indices
train_ds = Subset(train_full, train_idx)
val_ds   = Subset(val_full,   val_idx)

# 5) Dataloaders
BATCH = 2  # try to keep >=2 if possible
on_gpu = torch.cuda.is_available()
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0, pin_memory=on_gpu, drop_last=True)
val_loader   = DataLoader(val_ds,   batch_size=1,    shuffle=False, num_workers=0, pin_memory=on_gpu)

print(f"Train samples: {len(train_ds)} | Val samples: {len(val_ds)}")

def pixel_accuracy(pred: torch.Tensor, target: torch.Tensor, ignore_index: int = 255) -> float:
    valid = (target != ignore_index)
    correct = (pred == target) & valid
    denom = int(valid.sum().item())
    return float(correct.sum().item() / max(1, denom))

def mean_iou(pred: torch.Tensor, target: torch.Tensor, num_classes: int, ignore_index: int = 255) -> float:
    mask = (target != ignore_index)
    pred = pred[mask]
    target = target[mask]
    if target.numel() == 0:
        return 0.0
    k = (target * num_classes + pred).to(torch.int64)
    cm = torch.bincount(k, minlength=num_classes*num_classes).reshape(num_classes, num_classes).float()
    diag = torch.diag(cm)
    union = cm.sum(1) + cm.sum(0) - diag
    present = union > 0
    iou = torch.where(present, diag / torch.clamp(union, min=1.0), torch.zeros_like(union))
    return float(iou[present].mean().item()) if int(present.sum().item()) > 0 else 0.0

def train_one_epoch(model, loader, optimizer, device, epoch=1):
    model.train()
    total, n = 0.0, 0
    pbar = tqdm(loader, desc=f"epoch {epoch:02d}", leave=False)
    for imgs, labels in pbar:
        imgs   = imgs.to(device, non_blocking=True)    # [B,3,H,W]
        labels = labels.to(device, non_blocking=True)  # [B,H,W]
        optimizer.zero_grad(set_to_none=True)
        # Built-in CE loss when you pass labels=...
        out = model(pixel_values=imgs, labels=labels)
        loss = out.loss
        loss.backward()
        optimizer.step()

        bs = imgs.size(0)
        total += float(loss.item()) * bs
        n += bs
        pbar.set_postfix(loss=f"{loss.item():.4f}", avg=f"{total/max(1,n):.4f}")

    print(f"epoch {epoch:02d} | train_avg_loss {total/max(1,n):.4f}")
    return total / max(1, n)

@torch.no_grad()
def evaluate(model, loader, device, num_classes, ignore_index):
    model.eval()
    total, n = 0.0, 0
    acc_sum, iou_sum, m = 0.0, 0.0, 0
    for imgs, labels in loader:
        imgs   = imgs.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        out = model(pixel_values=imgs, labels=labels)
        loss = out.loss
        logits = out.logits                                # [B,K,h',w']
        # upsample logits to label size for fair metrics
        logits = F.interpolate(logits, size=labels.shape[-2:], mode="bilinear", align_corners=False)
        preds = logits.argmax(dim=1)                       # [B,H,W]

        acc_sum += pixel_accuracy(preds, labels, ignore_index)
        iou_sum += mean_iou(preds, labels, num_classes, ignore_index)

        bs = imgs.size(0)
        total += float(loss.item()) * bs
        n += bs; m += 1
    return total / max(1, n), acc_sum / max(1, m), iou_sum / max(1, m)

best_miou = -1.0
EPOCHS = 20
for epoch in range(1, EPOCHS+1):
    tr = train_one_epoch(model, train_loader, optimizer, device)
    va_loss, va_acc, va_miou = evaluate(model, val_loader, device, NUM_CLASSES, IGNORE_INDEX)
    print(f"Epoch {epoch:02d} | train {tr:.4f} | val {va_loss:.4f} | acc {va_acc:.3f} | mIoU {va_miou:.3f}")
    if va_miou > best_miou:
        best_miou = va_miou
        torch.save({
            "model_state": model.state_dict(),
            "num_classes": NUM_CLASSES,
            "ignore_index": IGNORE_INDEX,
            "backbone_id": "microsoft/swinv2-tiny-patch4-window8-256",
        }, "best_upernet_swinv2.pth")
        print("  âœ“ saved best_upernet_swinv2.pth")

model.eval()
with torch.no_grad():
    # grab one sample from val set
    img, _ = val_ds[0]                     # img: [3,H,W], normalized
    logits = model(pixel_values=img.unsqueeze(0).to(device)).logits  # [1,K,h',w']
    logits = F.interpolate(logits, size=img.shape[-2:], mode="bilinear", align_corners=False)
    pred = logits.argmax(dim=1)[0].cpu()   # [H,W] class IDs

# Optional: quick colorized viz
import numpy as np
from PIL import Image

def colorize_mask(mask_np, palette=None):
    if palette is None:
        rng = np.random.default_rng(123)
        K = int(mask_np.max()) + 1
        palette = [(int(rng.integers(0,256)), int(rng.integers(0,256)), int(rng.integers(0,256))) for _ in range(K)]
    h,w = mask_np.shape
    rgb = np.zeros((h,w,3), np.uint8)
    for k,(r,g,b) in enumerate(palette):
        rgb[mask_np==k] = (r,g,b)
    return Image.fromarray(rgb)

pred_np = pred.numpy().astype(np.uint8)
colorize_mask(pred_np).save("prediction_vis.png")

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image as PILImage  # avoid name clash with IPython.display.Image

IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])[:, None, None]
IMAGENET_STD  = np.array([0.229, 0.224, 0.225])[:, None, None]

def denorm_to_uint8(img_3chw):
    """img_3chw: torch tensor [3,H,W], ImageNet-normalized. -> np.uint8 [H,W,3]"""
    x = img_3chw.cpu().numpy()
    x = (x * IMAGENET_STD + IMAGENET_MEAN)
    x = np.clip(x * 255.0, 0, 255).astype(np.uint8)
    return np.transpose(x, (1, 2, 0))  # HWC

# 1) unnormalize original
orig = denorm_to_uint8(img)

# 2) make color mask from your prediction 'pred' (H,W)
pred_np = pred.numpy().astype(np.uint8)
color_mask = np.array(PILImage.open("prediction_vis.png"))  # you already saved it

# 3) overlay (alpha blend)
alpha = 0.55
overlay = (alpha * color_mask + (1 - alpha) * orig).astype(np.uint8)

# 4) show side-by-side
plt.figure(figsize=(18,6))
plt.subplot(1,3,1); plt.title("Original"); plt.imshow(orig); plt.axis("off")
plt.subplot(1,3,2); plt.title("Prediction (colorized)"); plt.imshow(color_mask); plt.axis("off")
plt.subplot(1,3,3); plt.title("Overlay"); plt.imshow(overlay); plt.axis("off")
plt.show()