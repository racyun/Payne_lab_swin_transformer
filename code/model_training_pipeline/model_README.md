# Swin Transformer UPerNet Model

## Swin Transformer
The Swin Transformer is a hierarchical vision backbone designed for high-resolution imagery and for seamless integration with multi-scale frameworks such as U-Net and Feature Pyramid Networks (FPN). Instead of processing the image at a single, fixed resolution, Swin constructs a pyramid of feature maps by first splitting the input into small non-overlapping patches that are linearly embedded into tokens, and then progressively merging neighboring tokens as depth increases. Early stages operate on a fine granularity by processing the dividing into small 4x4 pixel patches which allow the model to notice fine-grain, local details. Subsequent stages merge 2×2 groups of the resulting tokens by concatenating their features, temporarily expanding the channel dimension (e.g.  C → 4C). A linear projection then reduces this 4C vector to the next stage’s width (e.g., 2C—wider than the previous C but narrower than 4C), thereby reducing spatial resolution while enriching semantic abstraction. This stage-wise hierarchy closely mirrors classical CNNs, enabling effective representation learning across scales and making the backbone naturally compatible with dense prediction tasks such as detection and semantic segmentation.
<br>
Efficiency and long-range modeling are achieved through windowed self-attention with shifting. Standard Vision Transformers compute attention across all patches, which scales quadratically with image size; Swin, by contrast, restricts self-attention to fixed-size, non-overlapping windows so that the per-layer computation grows linearly with the number of pixels. To avoid isolating information within window boundaries, consecutive blocks alternate between regular Window-based Multi-Head Self-Attention (W-MSA) and Shifted-Window MSA (SW-MSA). A cyclic shift realigns the feature map so that shifted windows tile the space, while an attention mask prevents spurious interactions across disjoint regions created by the shift. Each block follows the standard Transformer block structure (see Fig. X), with relative-position bias in attention to encode spatial geometry. In practice, this combination of hierarchical patch merging, localized attention with linear complexity, and window shifting yields representations that preserve fine detail while aggregating broader context, supporting strong performance on image classification as well as multi-scale dense tasks.

## UNet Decoder
To turn the Swin feature pyramid into dense predictions, I adopt UPerNet, which couples a Pyramid Pooling Module (PPM) on the deepest backbone stage with an FPN-style top-down pathway and lateral connections. Specifically, global context features from the PPM (applied to the last Swin stage) are fused with progressively higher-resolution lateral features from earlier stages via 1×1 projections and 3×3 refinement blocks. The fused multi-scale maps are then upsampled and aggregated to produce a high-resolution feature map, followed by a lightweight segmentation head that outputs per-pixel class logits. This design preserves fine spatial detail while injecting large-receptive-field context, complementing Swin’s hierarchical, windowed self-attention.

## My Instantiation
In my model, I use a SwinV2-Tiny backbone (pretrained on ImageNet) as the encoder and feed the multi-scale feature maps produced at its four hierarchical stages into a UPerNet decoder. Specifically, the stages provide progressively coarser—but semantically richer—representations: Stage 0 (highest resolution, fewest channels; fine detail), Stage 1 (reduced spatial size, more channels; courser features), Stage 2 (further reduced spatial size, even more channels), and Stage 3 (lowest resolution, most channels; strongest semantics). UPerNet fuses these maps via a PPM on the deepest stage and an FPN-style top-down pathway with lateral connections, yielding a high-resolution aggregate feature that preserves detail while injecting global context. I train the primary head end-to-end for N classes with a cross-entropy loss, and at inference upsample the logits to the input resolution to produce final semantic masks.
